version: "2"
services:
  app:
    image: registry.gitlab.com/deepset-ai/open-source/fasttext-embeddings-de
    build: .
    command: bash -c "cd /home/docker/code/ && ./train_fasttext.sh"
    network_mode: "bridge"
    container_name: "fasttext"
    environment:
      # ** INPUT / OUTPUT DATA **
      CORPUS: input/wiki.de.txt
      DOWNLOAD_WIKI: "true" #downloads whole corpus. If corpus already extracted and processed (working wiki.de.txt file) you can set this to false
      CORPUS_TYPE: wiki
      OUTPUTDIR: output
      # ** MODEL PARAMETERS **
      # Min occurences of a token to be considered in the model vocab
      VOCAB_MIN_COUNT: 10
      # Dimensionality of word embedding
      VECTOR_SIZE: 300
      # Maximum number of training iterations (= epochs)
      MAX_ITER: 50
      # Number of words considered as a "local context" into both directions of the current target word
      # fasttext with skipgram implementation will predict a word out of this window using the "target" word
      WINDOW_SIZE: 5
      MAX_NGRAM_WORD: 1
      MAX_NGRAM_CHAR: 8
      MIN_NGRAM_CHAR: 3
      N_BUCKETS: 2000000
      SAMPLING_THRESH: 0.0001
      LEARNING_RATE: 0.05
      # ** RUNTIME SETTINGS**
      # Verbosity of logging (only increase for debugging)
      VERBOSE: 3
      THREADS: 16
      # ** META INFORMATION **
      #(will be stored after training in "word_emb.meta" file. Useful for inspecting, comparing and deploying models)
      TEXT_TYPE: "wiki"
      TOKEN_SPACER: "none"
      META_NOTES: "nothing"
    volumes:
        - ../data/input:/home/docker/code/input
        - ../data/output:/home/docker/code/output
